---
title: "R Notebook"
author: "Hengyang Lin hl3116"
output: html_notebook
---
##In this report, I will focus on what kind of people would gain happiness from a specific source. The sources of happiness could be approximately substituted by Topics when refer to happiness. And then I will explore the structure of the population whose people get happy from a specific sourece.

##The idea is simple: First we cluster the data to gain topics as substitutions of sources of happiness. Then we explore the population feature of each cluster.

#Step 0: Getting Data
Load libraries.
```{r, warning = FALSE}
packages.used <- c("tm", "wordcloud", "RColorBrewer", 
                "dplyr", "tydytext","topicmodels","qdap")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
library(ggplot2)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
library(tm)
library(topicmodels)
library(qdap)
```
Load the processed data & demographic data.
```{r load data, warning=FALSE, message=FALSE}
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm.data <- read_csv(urlfile) #load the raw data

corpus <- VCorpus(VectorSource(hm.data$cleaned_hm))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace) #clean data, remove punctuation, numbers,etc

stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)

dict <- tidy(corpus) %>%
  select(text) %>%
  unnest_tokens(dictionary, text)


data("stop_words")

word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past","day","time","spent","celebrated","fun","moment")

stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated")) #remove stopwords

completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) %>%
  anti_join(stop_words, by = c("dictionary" = "word"))

completed <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)

completed <- completed %>%
  group_by(id) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()

hm.data <- hm.data %>%
  mutate(id = row_number()) %>%
  inner_join(completed)


write_csv(hm.data, "../output/processed_moments.csv")


urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo.data <- read_csv(urlfile)
```
Merge the text data and demographic data.
```{r warning=FALSE, message=FALSE}
hm.data <- hm.data %>%
  inner_join(demo.data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         text) %>%
  mutate(count = sapply(hm.data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))

hm.data <- na.omit(hm.data)
```
#Step 1: Data Overview

Our data can be devided into different groups by different variable. Let's have a look at the whole data.

```{r, warning=FALSE}
#Female vs Male
ggplot(data = hm.data) +
  geom_bar(mapping = aes(x = gender, color = gender), fill = 2:3) +
  geom_text(aes(x = gender, label = ..count..), stat = "count", vjust = -0.1)
```
We have more male observations than female.

```{r,warning=FALSE}
#Maried vs Single
ggplot(data = hm.data) +
  geom_bar(mapping = aes(x = marital, color = marital), fill = 4:5) +
  geom_text(aes(x = marital, label = ..count..), stat = "count", vjust = -0.1)
```
We have more single people in the sample than married.

```{r,warning=FALSE}
#Age distribution
age <- as.numeric(hm.data$age)
hm.data <- hm.data[!is.na(age),]
ggplot(data = hm.data, aes(x = as.numeric(age), y = ..density..)) +
  geom_histogram(bins = 100) +
  scale_x_continuous(limits = c(0,90))+
  xlab("age")
```
It's easy to find out that the age is centered around 30 years old. The distribution of age is left skewed.

```{r,warning=FALSE}
#Country distribution
hm.data.split_country <- hm.data %>% group_by(country) %>%
  summarise(population_count = length(country))
ind <- order(hm.data.split_country$population_count, decreasing = TRUE)
head(hm.data.split_country[ind,],10)
```
We have listed 10 countries with most people in this sample.
We can learn that most people in this sample are from USA and India when people from USA constitute the majority. All other countries such as Canada and Venezuela contribute very little to this sample.

```{r,warning=FALSE}
#Parenthood: Yes or No
ggplot(data = hm.data) +
  geom_bar(mapping = aes(x = parenthood, color = parenthood), fill = 6:7) +
  geom_text(aes(x = parenthood, label = ..count..), stat = "count", vjust = -0.1)
```
We can see from the graph that there are more people without children in this sample.

```{r, warning=FALSE}
#Overall Wordcloud
alltext <- hm.data$text
text.df <- data_frame(obs = 1:length(alltext), text = alltext)
text.tidy <- text.df %>%
  unnest_tokens(word, text) #tokenization, seperate sentence to word

word_count <- text.tidy %>%
  count(word, sort = TRUE) #get a dataframe with count for each word

wordcloud(word_count$word, word_count$n,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```
This is the overall word cloud for the discription of happy moments. As we can see, "friend" is the most frequent word when people talk about happiness, followed with "family", "watched", etc. It seems that people get happy from doing something, since we have "watched" and "played" for frequent word, and also get happy from other people, such as "friend" and "family".

#Step 2: Topic Model

I would like to perform a topic model on this sample, because topic model helps us for understanding the main source of our happiness through clustering.
```{r, warning=FALSE}
docs <- Corpus(VectorSource(hm.data$text))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- paste(hm.data$gender, hm.data$marital,
                       hm.data$parenthood, hm.data$age, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
hm.data <- hm.data[rowTotals>0, ]

#Run LDA

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2736,47,193,658,1029)
nstart <- 5
best <- TRUE

#Number of topics
k <- 10

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))

#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut)) #result of clustering
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv"))

#top 20 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))#top 20 terms in each topic
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv"))

ldaOut.terms
```
Clearly we can find clues in the top 20 terms of each topics:

For topic 1, the terms strongly imply that these observations are clustered by family or relatives, which means one of the sources of happiness. So we assign hashtag "family" to Topic 1.

For topic 2, the terms involve "birthday", "party","marriage","attended",etc, which are mostly about social activities. So we assign hashtag "social activity" to Topic 2.

For topic 3, the terms involve "feel", "love", which are something about mood, as well as "morning", "night", "joy","smile","sleep","wake", which are about good life. So we assign hashtag "good mood and life" to Topic 3.

For topic 4, the terms strongly imply that these observations are clustered by career and money, which means one of the sources of happiness. So we assign hashtag "career" to Topic 4.

For topic 5, the main topic is quite blurred. It may involve being with others, inferred from "talked","spend","trip","parents",etc, also might be about success, which is deduced from "successfullt","didnt","havent". So we assign "accompany and success" to Topic 5.

For topic 6, it's very obvious that we could assign "food" to Topic 6.

For topic 7, it's very obvious that we could assign "education" to Topic 7.

For topic 8, it's very obvious that we could assign "shopping" to Topic 8.

For topic 9, the terms are about relaxing or vacation, so we could assign "vacation" to Topic 9.

For topic 10, it's very obvious that we could assign "sport" to Topic 10.

#Step 3: Analysis on each cluster
```{r, warning=FALSE}
topic_tag <- c("family","social activity","good mood and life","career","accompany and success",
               "food","education","shopping","vacation","sport")

#seperate the data into different clusters
ind.df <- read.csv("../output/LDAGibbs 10 DocsToTopics.csv")
int.mat <- matrix(NA, nrow = nrow(ind.df), ncol = 10)
colnames(int.mat) <- 1:10

for(i in 1:10){
  int.mat[,i] <- ind.df$V1 == i
}

pop1 <- hm.data[int.mat[,1],]
pop2 <- hm.data[int.mat[,2],]
pop3 <- hm.data[int.mat[,3],]
pop4 <- hm.data[int.mat[,4],]
pop5 <- hm.data[int.mat[,5],]
pop6 <- hm.data[int.mat[,6],]
pop7 <- hm.data[int.mat[,7],]
pop8 <- hm.data[int.mat[,8],]
pop9 <- hm.data[int.mat[,9],]
pop10 <- hm.data[int.mat[,10],]
```

Let's have a look about how big each cluster is.
```{r}

```